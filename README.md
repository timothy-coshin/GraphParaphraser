# Discrete Prompt Optimization for Knowledge Graph Question Answering

[![Hugging Face Collection](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Collection-blue)](https://huggingface.co/collections/CoShin/graph-paraphraser-gp)
[![Paper](https://img.shields.io/badge/Paper-WWW'26-red)](https://doi.org/10.1145/nnnnnnn.nnnnnnn)

Official repository for **Graph Paraphraser (GP)**, introduced in:

> **Graph Discrete Prompt Optimization for Knowledge Graph Question Answering**  
> *Proceedings of the ACM Web Conference 2026 (WWW â€™26)*

This repository releases:
- âœ… **Trained Graph Paraphraser models** (GP-SFT / GP-DPO)
- âœ… **Processed experimental datasets** used in the paper

All public artifacts are organized in the following Hugging Face collection:  
ðŸ‘‰ https://huggingface.co/collections/CoShin/graph-paraphraser-gp

---

## 1. Overview

Graph Paraphraser (GP) reformulates **graph-to-text conversion** as a  **discrete prompt optimization problem** for **frozen / black-box LLMs**.

Given:
- a natural language **question**
- retrieved **knowledge graph triples**

GP generates a **query-aware paraphrase prompt** that reorganizes the structured information to improve downstream QA accuracy when used with a frozen **Solver LLM**.

---

## 2. Model Naming Convention

All released models follow the naming rule:

```
CoShin/GP-{Paraphraser}-{Solver}-{Dataset}
```

### Example
```
CoShin/GP-LLaMa2-7B-L-Mintaka
```

| Component | Description |
|---------|-------------|
| `GP` | Graph Paraphraser |
| `LLaMa2-7B` | Paraphraser model|
| `L` / `M` | Solver (`L` = LLaMA, `M` = Mistral) |
| `Mintaka` | Dataset used for training |

Two variants are released:
- **GP-SFT**: supervised fine-tuning on successful paraphrases
- **GP-DPO**: preference-aligned model trained with Direct Preference Optimization (**recommended**)

---

## 3. Dataset Naming Convention

All datasets follow:

```
CoShin/GP-{Dataset}-{Paraphraser}-{Solver}
```

### Example
```
CoShin/GP-Mintaka-LLaMa2-LLaMa2
```

---

## 4. Dataset Schema

| Column | Description |
|------|-------------|
| question | Natural language question |
| label | Ground-truth short answer |
| linearization | Linearized KG triples |
| hint_doc | Free-form hint paragraph generated by a frozen LLM |
| sft_result | Paraphrase generated by **GP-SFT** |
| dpo_result | Paraphrase generated by **GP-DPO** |

---

## 5. Original Data Sources

- Mintaka: https://github.com/amazon-science/mintaka  
- Wikidata SimpleQuestions: https://github.com/askplatypus/wikidata-simplequestions  
- WebQSP: https://github.com/UKPLab/coling2018-graph-neural-networks-question-answering/blob/master/WEBQSP_WD_README.md  

---

## 6. Prompt Templates Used

### Hint Paragraph Generation

```python
def hint_paragraph_prompting(question, linearized_triple):
    res = (
        f'### Please create a short hint paragraph to answer the question reorganizing the triple information, step by step.\n'
        f'### Question: {question}\n'
        f'### Triple Information: {linearized_triple}\n'
        f'### Hint Paragraph:'
    )
    return res
```

### Answer Generation Prompt

```python
def answer_prompting(question, hint):
    hint = hint.replace('\n', ' ')
    res = (
        f'### Below are the facts that might be relevant to answer the question. '
        f'Please provide a short answer (1-3 words in English) to the following question.\n'
        f'### Facts: {hint.strip()}\n'
        f'### Question: {question.strip()}\n'
        f'### Answer:'
    )
    return res
```

---

## 7. Citation

The paper link, DOI, and official citation information will be released shortly
and will be updated in this repository once they become publicly available.

---

## 8. Contact

- Wooyoung Kim â€” timothy@yonsei.ac.kr  

